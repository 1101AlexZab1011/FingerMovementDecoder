{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.path.dirname(os.path.abspath('./'))\n",
    "if not current_dir in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "current_dir = os.path.dirname(os.path.abspath('../'))\n",
    "if not current_dir in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "\n",
    "from utils.structures import Pipeline, Deploy\n",
    "from utils.data_management import dict2str\n",
    "from utils.machine_learning import one_hot_encoder, one_hot_decoder\n",
    "from typing import *\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import make_classification\n",
    "import mne\n",
    "from combiners import EpochsCombiner\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from utils.machine_learning.designer import ModelDesign, ParallelDesign, LayerDesign\n",
    "from utils.machine_learning.analyzer import ModelAnalyzer, LFCNNAnalyzer\n",
    "from mne.datasets import multimodal\n",
    "import sklearn\n",
    "import mneflow as mf\n",
    "import tensorflow as tf\n",
    "from mneflow.layers import DeMixing, LFTConv, TempPooling, Dense\n",
    "from mneflow.models import BaseModel\n",
    "import mneflow\n",
    "import logging\n",
    "from time import perf_counter\n",
    "\n",
    "logger= logging.getLogger(__name__)\n",
    "logging.root.handlers = []\n",
    "logger.setLevel(logging.NOTSET)\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s, %(name)s %(levelname)s %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    level=logging.DEBUG,\n",
    "    handlers=[\n",
    "        logging.FileHandler('./history.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deconw(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units=32,\n",
    "        kernel_size=(4, 10),\n",
    "        strides=(1, 1),\n",
    "        padding='valid',\n",
    "        output_padding=None,\n",
    "        data_format=None,\n",
    "        dilation_rate=(1, 1),\n",
    "        activation=None,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.output_padding = output_padding\n",
    "        self.data_format = data_format\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "        self.activity_regularizer = activity_regularizer\n",
    "        self.kernel_constraint = kernel_constraint\n",
    "        self.bias_constraint = bias_constraint\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.n_channels = input_shape[-1]\n",
    "        self.deconws = self.deconv_constructor(self.n_channels)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        self.build(inputs.shape)\n",
    "        outputs = []\n",
    "        for i in range(self.n_channels):\n",
    "            input_ = tf.expand_dims(inputs[:, :, :, i], axis=3)\n",
    "            outputs.append(self.deconws[i](input_))\n",
    "        return tf.transpose(tf.stack(outputs), (1, 0, 2, 3, 4))\n",
    "\n",
    "    def deconv_constructor(self, n_channels):\n",
    "        return [\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=1,\n",
    "                kernel_size=self.kernel_size,\n",
    "                strides=self.strides,\n",
    "                padding=self.padding,\n",
    "                output_padding=self.output_padding,\n",
    "                data_format=self.data_format,\n",
    "                dilation_rate=self.dilation_rate,\n",
    "                activation=self.activation,\n",
    "                use_bias=self.use_bias,\n",
    "                kernel_initializer=self.kernel_initializer,\n",
    "                bias_initializer=self.bias_initializer,\n",
    "                kernel_regularizer=self.kernel_regularizer,\n",
    "                bias_regularizer=self.bias_regularizer,\n",
    "                activity_regularizer=self.activity_regularizer,\n",
    "                kernel_constraint=self.kernel_constraint,\n",
    "                bias_constraint=self.bias_constraint,\n",
    "                **self.kwargs\n",
    "            )\n",
    "        for _ in range(n_channels)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.set_log_level(verbose='CRITICAL')\n",
    "fname_raw = os.path.join(multimodal.data_path(), 'multimodal_raw.fif')\n",
    "raw = mne.io.read_raw_fif(fname_raw)\n",
    "cond = raw.acqparser.get_condition(raw, None)\n",
    "condition_names = [k for c in cond for k,v in c['event_id'].items()]\n",
    "epochs_list = [mne.Epochs(raw, **c) for c in cond]\n",
    "epochs = mne.concatenate_epochs(epochs_list)\n",
    "epochs = epochs.pick_types(meg='grad')\n",
    "X = np.array([])\n",
    "Y = list()\n",
    "for i, epochs in enumerate(epochs_list):\n",
    "    data = epochs.get_data()\n",
    "    if i == 0:\n",
    "        X = data.copy()\n",
    "    else:\n",
    "        X = np.append(X, data, axis=0)\n",
    "    Y += [i for _ in range(data.shape[0])]\n",
    "\n",
    "Y = np.array(Y)\n",
    "X = np.array([X[i, epochs._channel_type_idx['grad'], :] for i, _ in enumerate(X)])\n",
    "original_X = X.copy()\n",
    "original_Y = Y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_opt = dict(savepath='../tfr/',\n",
    "    out_name='mne_sample_epochs',\n",
    "    fs=600,\n",
    "    input_type='trials',\n",
    "    target_type='int',\n",
    "    picks={'meg':'grad'},\n",
    "    scale=True,  # apply baseline_scaling\n",
    "    crop_baseline=True,  # remove baseline interval after scaling\n",
    "    decimate=None,\n",
    "    scale_interval=(0, 60),  # indices in time axis corresponding to baseline interval\n",
    "    n_folds=5,\n",
    "    overwrite=True,\n",
    "    segment=False,\n",
    ")\n",
    "\n",
    "specs = dict()\n",
    "specs.setdefault('filter_length', 7)\n",
    "specs.setdefault('n_latent', 4)\n",
    "specs.setdefault('pooling', 10)\n",
    "specs.setdefault('stride', 2)\n",
    "specs.setdefault('padding', 'SAME')\n",
    "specs.setdefault('pool_type', 'max')\n",
    "specs.setdefault('nonlin', tf.nn.relu)\n",
    "specs.setdefault('l1', 3e-4)\n",
    "specs.setdefault('l2', 0)\n",
    "specs.setdefault('l1_scope', ['fc', 'dmx', 'tconv', 'fc'])\n",
    "specs.setdefault('l2_scope', [])\n",
    "specs.setdefault('maxnorm_scope', [])\n",
    "specs.setdefault('dropout', .5)\n",
    "\n",
    "specs['filter_length'] = 17\n",
    "specs['pooling'] = 5\n",
    "specs['stride'] = 5\n",
    "specs['l1'] = 3e-3\n",
    "# out_dim = len(np.unique(original_Y))\n",
    "# Y = original_Y.copy()\n",
    "# Y = one_hot_encoder(Y)\n",
    "# X = original_X.copy()\n",
    "# X = np.transpose(np.expand_dims(X, axis = 1), (0, 1, 3, 2))\n",
    "# print(X.shape)\n",
    "# n_samples, _, n_times, n_channels = X.shape\n",
    "# X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(original_X, original_Y, train_size=.85)\n",
    "\n",
    "# # write TFRecord files and metadata file to disk\n",
    "# meta = mneflow.produce_tfrecords((original_X, original_Y), **import_opt)\n",
    "# dataset = mneflow.Dataset(meta, train_batch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting reg for fc, to l1\n"
     ]
    }
   ],
   "source": [
    "out_dim=8\n",
    "\n",
    "lfcnnd = ModelDesign(\n",
    "    None,\n",
    "    DeMixing(\n",
    "        size=specs['n_latent'],\n",
    "        nonlin=tf.identity,\n",
    "        axis=3, specs=specs\n",
    "    ),\n",
    "    LFTConv(\n",
    "        size=specs['n_latent'],\n",
    "        nonlin=specs['nonlin'],\n",
    "        filter_length=specs['filter_length'],\n",
    "        padding=specs['padding'],\n",
    "        specs=specs\n",
    "    ),\n",
    "    TempPooling(\n",
    "        pooling=specs['pooling'],\n",
    "        pool_type=specs['pool_type'],\n",
    "        stride=specs['stride'],\n",
    "        padding=specs['padding'],\n",
    "    ),\n",
    "    tf.keras.layers.Dropout(specs['dropout'], noise_shape=None),\n",
    "    Dense(size=out_dim, nonlin=tf.identity, specs=specs)\n",
    ")\n",
    "\n",
    "\n",
    "class ZubarevBaseNet(BaseModel):\n",
    "    def __init__(self, Dataset, specs=dict()):\n",
    "        super().__init__(Dataset, specs)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        n_epochs,\n",
    "        eval_step=None,\n",
    "        min_delta=1e-6,\n",
    "        early_stopping=3,\n",
    "        mode='single_fold',\n",
    "        *,\n",
    "        callbacks=None\n",
    "    ):\n",
    "        callbacks = [] if callbacks is None else callbacks\n",
    "\n",
    "        stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=min_delta,\n",
    "            patience=early_stopping,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        if not eval_step:\n",
    "            train_size = self.dataset.h_params['train_size']\n",
    "            eval_step = train_size // self.dataset.h_params['train_batch'] + 1\n",
    "\n",
    "        self.train_params = [n_epochs, eval_step, early_stopping, mode]\n",
    "\n",
    "        if mode == 'single_fold':\n",
    "            self.t_hist = self.km.fit(\n",
    "                self.dataset.train,\n",
    "                validation_data=self.dataset.val,\n",
    "                epochs=n_epochs, steps_per_epoch=eval_step,\n",
    "                shuffle=True,\n",
    "                validation_steps=self.dataset.validation_steps,\n",
    "                callbacks=[stop_early, *callbacks], verbose=2\n",
    "            )\n",
    "            self.v_loss, self.v_metric = self.evaluate(self.dataset.val)\n",
    "            self.v_loss_sd = 0\n",
    "            self.v_metric_sd = 0\n",
    "            print(\"Training complete: loss: {}, Metric: {}\".format(self.v_loss, self.v_metric))\n",
    "            self.update_log()\n",
    "        elif mode == 'cv':\n",
    "            n_folds = len(self.dataset.h_params['folds'][0])\n",
    "            print(\"Running cross-validation with {} folds\".format(n_folds))\n",
    "            metrics = []\n",
    "            losses = []\n",
    "            for jj in range(n_folds):\n",
    "                print(\"fold:\", jj)\n",
    "                train, val = self.dataset._build_dataset(\n",
    "                    self.dataset.h_params['train_paths'],\n",
    "                    train_batch=self.dataset.training_batch,\n",
    "                    test_batch=self.dataset.validation_batch,\n",
    "                    split=True, val_fold_ind=jj\n",
    "                )\n",
    "                self.t_hist = self.km.fit(\n",
    "                    train,\n",
    "                    validation_data=val,\n",
    "                    epochs=n_epochs, steps_per_epoch=eval_step,\n",
    "                    shuffle=True,\n",
    "                    validation_steps=self.dataset.validation_steps,\n",
    "                    callbacks=[stop_early, *callbacks], verbose=2\n",
    "                )\n",
    "                loss, metric = self.evaluate(val)\n",
    "                losses.append(loss)\n",
    "                metrics.append(metric)\n",
    "\n",
    "                if jj < n_folds -1:\n",
    "                    self.shuffle_weights()\n",
    "                else:\n",
    "                    \"Not shuffling the weights for the last fold\"\n",
    "\n",
    "\n",
    "                print(\"Fold: {} Loss: {:.4f}, Metric: {:.4f}\".format(jj, loss, metric))\n",
    "            self.cv_losses = losses\n",
    "            self.cv_metrics = metrics\n",
    "            self.v_loss = np.mean(losses)\n",
    "            self.v_metric = np.mean(metrics)\n",
    "            self.v_loss_sd = np.std(losses)\n",
    "            self.v_metric_sd = np.std(metrics)\n",
    "            print(\"{} with {} folds completed. Loss: {:.4f} +/- {:.4f}. Metric: {:.4f} +/- {:.4f}\".format(mode, n_folds, np.mean(losses), np.std(losses), np.mean(metrics), np.std(metrics)))\n",
    "            self.update_log()\n",
    "            return self.cv_losses, self.cv_metrics\n",
    "\n",
    "        elif mode == \"loso\":\n",
    "            n_folds = len(self.dataset.h_params['test_paths'])\n",
    "            print(\"Running leave-one-subject-out CV with {} subject\".format(n_folds))\n",
    "            metrics = []\n",
    "            losses = []\n",
    "            for jj in range(n_folds):\n",
    "                print(\"fold:\", jj)\n",
    "\n",
    "                test_subj = self.dataset.h_params['test_paths'][jj]\n",
    "                train_subjs = self.dataset.h_params['train_paths'].copy()\n",
    "                train_subjs.pop(jj)\n",
    "\n",
    "                train, val = self.dataset._build_dataset(\n",
    "                    train_subjs,\n",
    "                    train_batch=self.dataset.training_batch,\n",
    "                    test_batch=self.dataset.validation_batch,\n",
    "                    split=True, val_fold_ind=0\n",
    "                )\n",
    "                self.t_hist = self.km.fit(\n",
    "                    train,\n",
    "                    validation_data=val,\n",
    "                    epochs=n_epochs, steps_per_epoch=eval_step,\n",
    "                    shuffle=True,\n",
    "                    validation_steps=self.dataset.validation_steps,\n",
    "                    callbacks=[stop_early, *callbacks], verbose=2\n",
    "                )\n",
    "                test = self.dataset._build_dataset(\n",
    "                    test_subj,\n",
    "                    test_batch=None,\n",
    "                    split=False\n",
    "                )\n",
    "\n",
    "                loss, metric = self.evaluate(test)\n",
    "                losses.append(loss)\n",
    "                metrics.append(metric)\n",
    "\n",
    "                if jj < n_folds -1:\n",
    "                    self.shuffle_weights()\n",
    "                else:\n",
    "                    \"Not shuffling the weights for the last fold\"\n",
    "\n",
    "            self.cv_losses = losses\n",
    "            self.cv_metrics = metrics\n",
    "            self.v_loss = np.mean(losses)\n",
    "            self.v_metric = np.mean(metrics)\n",
    "            self.v_loss_sd = np.std(losses)\n",
    "            self.v_metric_sd = np.std(metrics)\n",
    "            self.update_log()\n",
    "            print(\"{} with {} folds completed. Loss: {:.4f} +/- {:.4f}. Metric: {:.4f} +/- {:.4f}\".format(mode, n_folds, np.mean(losses), np.std(losses), np.mean(metrics), np.std(metrics)))\n",
    "            return self.cv_losses, self.cv_metrics\n",
    "\n",
    "\n",
    "class ZubarevNet(ZubarevBaseNet):\n",
    "    def __init__(self, Dataset, specs=dict(), design=lfcnnd, design_name='design'):\n",
    "        self.scope = design_name\n",
    "        self.design = design\n",
    "        specs.setdefault('filter_length', 7)\n",
    "        specs.setdefault('n_latent', 4)\n",
    "        specs.setdefault('pooling', 4)\n",
    "        specs.setdefault('stride', 4)\n",
    "        specs.setdefault('padding', 'SAME')\n",
    "        specs.setdefault('pool_type', 'max')\n",
    "        specs.setdefault('nonlin', tf.nn.relu)\n",
    "        specs.setdefault('l1', 3e-4)\n",
    "        specs.setdefault('l2', 0)\n",
    "        specs.setdefault('l1_scope', ['fc', 'demix', 'lf_conv'])\n",
    "        specs.setdefault('l2_scope', [])\n",
    "        specs.setdefault('maxnorm_scope', [])\n",
    "\n",
    "        super().__init__(Dataset, specs)\n",
    "\n",
    "    def build_graph(self):\n",
    "        return self.design(self.inputs)\n",
    "\n",
    "    def set_design(self, design: ModelDesign):\n",
    "        self.design = design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting reg for fc, to l1\n",
      "Setting reg for fc, to l1\n",
      "Setting reg for fc, to l1\n"
     ]
    }
   ],
   "source": [
    "simplenetd = ModelDesign(\n",
    "    tf.keras.Input(shape=(1, n_times, n_channels)),\n",
    "    DeMixing(\n",
    "        size=specs['n_latent'],\n",
    "        nonlin=tf.identity,\n",
    "        axis=3, specs=specs\n",
    "    ),\n",
    "    LFTConv(\n",
    "        size=specs['n_latent'],\n",
    "        nonlin=specs['nonlin'],\n",
    "        filter_length=specs['filter_length'],\n",
    "        padding=specs['padding'],\n",
    "        specs=specs\n",
    "    ),\n",
    "    LFTConv(\n",
    "        size=specs['n_latent'],\n",
    "        nonlin=specs['nonlin'],\n",
    "        filter_length=specs['filter_length'],\n",
    "        padding=specs['padding'],\n",
    "        specs=specs\n",
    "    ),\n",
    "    LayerDesign(\n",
    "        lambda X: X[:, :, ::2, :]\n",
    "    ),\n",
    "    tf.keras.layers.Dropout(specs['dropout'], noise_shape=None),\n",
    "    Dense(size=out_dim, nonlin=tf.identity, specs=specs)\n",
    ")\n",
    "\n",
    "lfrnnd = ModelDesign(\n",
    "    None,\n",
    "    LayerDesign(tf.squeeze, axis=1),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(\n",
    "            specs['n_latent'],\n",
    "            bias_regularizer='l1',\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=tf.keras.regularizers.L1(.01),\n",
    "            recurrent_regularizer=tf.keras.regularizers.L1(.01),\n",
    "            dropout=0.4,\n",
    "            recurrent_dropout=0.4,\n",
    "        ),\n",
    "        merge_mode='sum'\n",
    "    ),\n",
    "    LayerDesign(tf.expand_dims, axis=1),\n",
    "    LFTConv(\n",
    "        size=specs['n_latent'],\n",
    "        nonlin=specs['nonlin'],\n",
    "        filter_length=specs['filter_length'],\n",
    "        padding=specs['padding'],\n",
    "        specs=specs\n",
    "    ),\n",
    "    TempPooling(\n",
    "        pooling=specs['pooling'],\n",
    "        pool_type=specs['pool_type'],\n",
    "        stride=specs['stride'],\n",
    "        padding=specs['padding'],\n",
    "    ),\n",
    "    tf.keras.layers.Dropout(specs['dropout'], noise_shape=None),\n",
    "    Dense(size=out_dim, nonlin=tf.identity, specs=specs)\n",
    ")\n",
    "\n",
    "\n",
    "# newnetd = ModelDesign(\n",
    "#     tf.keras.Input(shape=(1, n_times, n_channels)),\n",
    "#     Deconw(kernel_size=(specs['n_latent'], specs['filter_length']), activation='relu', kernel_regularizer='l1'),\n",
    "#     tf.keras.layers.Conv2D(1, (1, specs['filter_length']), activation='relu', kernel_regularizer='l2'),\n",
    "#     LayerDesign(\n",
    "#         lambda X: tf.transpose(tf.squeeze(X, axis=-1), (0, 2, 3, 1))\n",
    "#     ),\n",
    "#     tf.keras.layers.Conv2D(1, (1, 204), padding='same'),\n",
    "#     LayerDesign(\n",
    "#         lambda X: tf.transpose(X, (0, 3, 2, 1))\n",
    "#     ),\n",
    "#     LayerDesign(\n",
    "#         lambda X: X[:, :, ::specs['pooling'], :]\n",
    "#     ),\n",
    "#     tf.keras.layers.Dropout(specs['dropout'], noise_shape=None),\n",
    "#     Dense(size=out_dim, nonlin=tf.identity, specs=specs)\n",
    "# )\n",
    "\n",
    "# newnetd = ModelDesign(\n",
    "#     tf.keras.Input(shape=(1, n_times, n_channels)),\n",
    "#     Deconw(kernel_size=(specs['n_latent'], specs['filter_length']), activation='relu', kernel_regularizer='l1'),\n",
    "#     tf.keras.layers.Conv2D(1, (1, specs['filter_length']), activation='relu'),\n",
    "#     LayerDesign(\n",
    "#         lambda X: tf.transpose(tf.squeeze(X, axis=-1), (0, 1, 3, 2))\n",
    "#     ),\n",
    "#     tf.keras.layers.DepthwiseConv2D((204, 1), kernel_regularizer='l1'),\n",
    "#     LayerDesign(\n",
    "#         lambda X: X[:, :, ::2, :]\n",
    "#     ),\n",
    "#     tf.keras.layers.Dropout(specs['dropout'], noise_shape=None),\n",
    "#     Dense(size=out_dim, nonlin=tf.identity, specs=specs)\n",
    "# )\n",
    "\n",
    "newnetd = ModelDesign(\n",
    "    tf.keras.Input(shape=(1, n_times, n_channels)),\n",
    "    Deconw(kernel_size=(specs['n_latent'], specs['filter_length']), activation='relu', kernel_regularizer='l1'),\n",
    "    LayerDesign(\n",
    "        lambda X: tf.transpose(tf.squeeze(X, axis=-1), (0, 1, 3, 2))\n",
    "    ),\n",
    "    tf.keras.layers.DepthwiseConv2D((1, specs['filter_length']), activation='relu', depthwise_regularizer='l1'),\n",
    "    tf.keras.layers.DepthwiseConv2D((n_channels, 1), name='demixing', depthwise_regularizer='l1'),\n",
    "    LayerDesign(\n",
    "        lambda X: X[:, :, ::2, :]\n",
    "    ),\n",
    "    tf.keras.layers.Dropout(specs['dropout'], noise_shape=None),\n",
    "    Dense(size=out_dim, nonlin=tf.identity, specs=specs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:34:15, git.cmd DEBUG Popen(['git', 'version'], cwd=/home/user/Projects/FingerMovementDecoder/dirty_field/net_dev, universal_newlines=False, shell=None, istream=None)\n",
      "18:34:15, git.cmd DEBUG Popen(['git', 'version'], cwd=/home/user/Projects/FingerMovementDecoder/dirty_field/net_dev, universal_newlines=False, shell=None, istream=None)\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "class WanbCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        meta,\n",
    "        *args, **kwargs):\n",
    "        self.model = model\n",
    "        self.meta = meta\n",
    "        self.start_time = perf_counter()\n",
    "        wandb.init(*args, **kwargs)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        wandb.log(logs)\n",
    "    def on_train_end(self, logs=None):\n",
    "        train_runtime = perf_counter() - self.start_time\n",
    "        wandb.log(dict(\n",
    "            train_runtime=train_runtime\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting reg for dmx, to l1\n",
      "Built: dmx input: (None, 1, 361, 204)\n",
      "Setting reg for tconv, to l1\n",
      "Built: tconv input: (None, 1, 361, 4)\n",
      "Setting reg for tconv, to l1\n",
      "Built: tconv input: (None, 1, 361, 4)\n",
      "Built: fc input: (None, 1, 181, 4)\n",
      "Input shape: (1, 361, 204)\n",
      "y_pred: (None, 8)\n",
      "Initialization complete!\n",
      "18:34:16, wandb.jupyter ERROR Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "18:34:16, urllib3.connectionpool DEBUG Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "18:34:17, urllib3.connectionpool DEBUG https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 None\n",
      "18:34:17, urllib3.connectionpool DEBUG Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "18:34:17, urllib3.connectionpool DEBUG https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexzab\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:34:17, git.cmd DEBUG Popen(['git', 'cat-file', '--batch-check'], cwd=/home/user/Projects/FingerMovementDecoder, universal_newlines=False, shell=None, istream=<valid stream>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/Projects/FingerMovementDecoder/dirty_field/net_dev/wandb/run-20220828_183417-1zesndz4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alexzab/fmdec/runs/1zesndz4\" target=\"_blank\">simplenet</a></strong> to <a href=\"https://wandb.ai/alexzab/fmdec\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "100/100 - 4s - loss: 3.2023 - cat_ACC: 0.1462 - val_loss: 3.1243 - val_cat_ACC: 0.1330 - 4s/epoch - 43ms/step\n",
      "Epoch 2/25\n",
      "100/100 - 3s - loss: 3.0497 - cat_ACC: 0.1801 - val_loss: 3.0539 - val_cat_ACC: 0.1436 - 3s/epoch - 28ms/step\n",
      "Epoch 3/25\n",
      "100/100 - 3s - loss: 2.9476 - cat_ACC: 0.2106 - val_loss: 2.9896 - val_cat_ACC: 0.1596 - 3s/epoch - 28ms/step\n",
      "Epoch 4/25\n",
      "100/100 - 3s - loss: 2.8510 - cat_ACC: 0.2413 - val_loss: 2.9109 - val_cat_ACC: 0.1915 - 3s/epoch - 26ms/step\n",
      "Epoch 5/25\n",
      "100/100 - 3s - loss: 2.7248 - cat_ACC: 0.2976 - val_loss: 2.7746 - val_cat_ACC: 0.2128 - 3s/epoch - 28ms/step\n",
      "Epoch 6/25\n",
      "100/100 - 3s - loss: 2.5037 - cat_ACC: 0.3720 - val_loss: 2.5968 - val_cat_ACC: 0.2872 - 3s/epoch - 27ms/step\n",
      "Epoch 7/25\n",
      "100/100 - 3s - loss: 2.3333 - cat_ACC: 0.4248 - val_loss: 2.4594 - val_cat_ACC: 0.3564 - 3s/epoch - 29ms/step\n",
      "Epoch 8/25\n",
      "100/100 - 3s - loss: 2.1269 - cat_ACC: 0.5025 - val_loss: 2.3434 - val_cat_ACC: 0.3989 - 3s/epoch - 29ms/step\n",
      "Epoch 9/25\n",
      "100/100 - 3s - loss: 1.9690 - cat_ACC: 0.5522 - val_loss: 2.2327 - val_cat_ACC: 0.4096 - 3s/epoch - 32ms/step\n",
      "Epoch 10/25\n",
      "100/100 - 3s - loss: 1.7961 - cat_ACC: 0.6185 - val_loss: 2.1095 - val_cat_ACC: 0.4734 - 3s/epoch - 34ms/step\n",
      "Epoch 11/25\n",
      "100/100 - 3s - loss: 1.6574 - cat_ACC: 0.6672 - val_loss: 1.9807 - val_cat_ACC: 0.5106 - 3s/epoch - 28ms/step\n",
      "Epoch 12/25\n",
      "100/100 - 3s - loss: 1.5140 - cat_ACC: 0.7162 - val_loss: 1.8732 - val_cat_ACC: 0.5638 - 3s/epoch - 27ms/step\n",
      "Epoch 13/25\n",
      "100/100 - 3s - loss: 1.4119 - cat_ACC: 0.7542 - val_loss: 1.7894 - val_cat_ACC: 0.5957 - 3s/epoch - 27ms/step\n",
      "Epoch 14/25\n",
      "100/100 - 3s - loss: 1.3066 - cat_ACC: 0.7842 - val_loss: 1.7166 - val_cat_ACC: 0.6383 - 3s/epoch - 29ms/step\n",
      "Epoch 15/25\n",
      "100/100 - 3s - loss: 1.2220 - cat_ACC: 0.8172 - val_loss: 1.6423 - val_cat_ACC: 0.6543 - 3s/epoch - 31ms/step\n",
      "Epoch 16/25\n",
      "100/100 - 3s - loss: 1.1466 - cat_ACC: 0.8388 - val_loss: 1.5753 - val_cat_ACC: 0.6649 - 3s/epoch - 30ms/step\n",
      "Epoch 17/25\n",
      "100/100 - 3s - loss: 1.0924 - cat_ACC: 0.8522 - val_loss: 1.5331 - val_cat_ACC: 0.6809 - 3s/epoch - 29ms/step\n",
      "Epoch 18/25\n",
      "100/100 - 3s - loss: 1.0273 - cat_ACC: 0.8743 - val_loss: 1.4872 - val_cat_ACC: 0.6968 - 3s/epoch - 28ms/step\n",
      "Epoch 19/25\n",
      "100/100 - 3s - loss: 0.9921 - cat_ACC: 0.8819 - val_loss: 1.4880 - val_cat_ACC: 0.7287 - 3s/epoch - 28ms/step\n",
      "Epoch 20/25\n",
      "100/100 - 3s - loss: 0.9360 - cat_ACC: 0.8995 - val_loss: 1.4283 - val_cat_ACC: 0.7287 - 3s/epoch - 28ms/step\n",
      "Epoch 21/25\n",
      "100/100 - 3s - loss: 0.8959 - cat_ACC: 0.9084 - val_loss: 1.3966 - val_cat_ACC: 0.7394 - 3s/epoch - 28ms/step\n",
      "Epoch 22/25\n",
      "100/100 - 3s - loss: 0.8612 - cat_ACC: 0.9172 - val_loss: 1.3699 - val_cat_ACC: 0.7340 - 3s/epoch - 30ms/step\n",
      "Epoch 23/25\n",
      "100/100 - 3s - loss: 0.8289 - cat_ACC: 0.9236 - val_loss: 1.3626 - val_cat_ACC: 0.7606 - 3s/epoch - 30ms/step\n",
      "Epoch 24/25\n",
      "100/100 - 3s - loss: 0.7944 - cat_ACC: 0.9284 - val_loss: 1.3434 - val_cat_ACC: 0.7500 - 3s/epoch - 31ms/step\n",
      "Epoch 25/25\n",
      "100/100 - 3s - loss: 0.7710 - cat_ACC: 0.9334 - val_loss: 1.3312 - val_cat_ACC: 0.7713 - 3s/epoch - 30ms/step\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.3312 - cat_ACC: 0.7713\n",
      "Training complete: loss: 1.3311665058135986, Metric: 0.771276593208313\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6525 - cat_ACC: 1.0000\n",
      "No dataset specified using validation dataset (Default)\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 1.3312 - cat_ACC: 0.7713\n",
      "Updating log: test loss: 1.3312 test metric: 0.7713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 18:35:32.665895: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 276901440 exceeds 10% of free system memory.\n",
      "2022-08-28 18:35:33.037427: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 276901440 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No dataset specified using validation dataset (Default)\n",
      "18:35:35, root INFO simplenet performance:\n",
      "\truntime:  1.6491\n",
      "\ttrain-set: 0.948936170212766\n",
      "\ttest-set: 0.7712765957446809\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# model = ZubarevNet(dataset, specs, lfcnnd, 'lfcnn')\n",
    "model_name='simplenet'\n",
    "model = ZubarevNet(dataset, specs, simplenetd, model_name)\n",
    "model.build()\n",
    "t1 = perf_counter()\n",
    "model.train(n_epochs=25, eval_step=100, early_stopping=5,\n",
    "            callbacks=[\n",
    "                WanbCallback(\n",
    "                    model, meta,\n",
    "                    project='fmdec',\n",
    "                    config=specs,\n",
    "                    name=model_name\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "y_true_train, y_pred_train = model.predict(meta['train_paths'])\n",
    "t1 = perf_counter()\n",
    "y_true_test, y_pred_test = model.predict(meta['test_paths'])\n",
    "runtime=perf_counter()-t1\n",
    "logging.info(\n",
    "    f'{model.scope} performance:\\n'\n",
    "    f'\\truntime: {runtime : .4f}\\n'\n",
    "    f'\\ttrain-set: {sklearn.metrics.accuracy_score(one_hot_decoder(y_true_train), one_hot_decoder(y_pred_train))}\\n'\n",
    "    f'\\ttest-set: {sklearn.metrics.accuracy_score(one_hot_decoder(y_true_test), one_hot_decoder(y_pred_test))}'\n",
    ")\n",
    "\n",
    "wandb.log(dict(\n",
    "    test_runtime=runtime,\n",
    "    train_acc=sklearn.metrics.accuracy_score(one_hot_decoder(y_true_train), one_hot_decoder(y_pred_train)),\n",
    "    test_acc=sklearn.metrics.accuracy_score(one_hot_decoder(y_true_test), one_hot_decoder(y_pred_test))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting reg for fc, to l1\n",
      "input_shape:  (1, 361, 204)\n",
      "Built: fc input: (None, 1, 181, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples, _, n_times, n_channels = (940, 1, 361, 204)\n",
    "out_dim=8\n",
    "\n",
    "# newnetd = ModelDesign(\n",
    "#     tf.keras.Input(shape=(1, n_times, n_channels)),\n",
    "#     Deconw(kernel_size=(specs['n_latent'], specs['filter_length']), activation='relu', kernel_regularizer='l1'),\n",
    "#     tf.keras.layers.Conv2D(1, (1, specs['filter_length']), activation='relu', kernel_regularizer='l2'),\n",
    "#     LayerDesign(\n",
    "#         lambda X: tf.transpose(tf.squeeze(X, axis=-1), (0, 1, 3, 2))\n",
    "#     ),\n",
    "#     # TensorShape([None, 4, 361, 204])\n",
    "#     tf.keras.layers.Conv2D(1, (204, 1), padding='valid', name='demixing'),\n",
    "#     # LayerDesign(\n",
    "#     #     lambda X: tf.transpose(X, (0, 3, 2, 1))\n",
    "#     # ),\n",
    "#     # LayerDesign(\n",
    "#     #     lambda X: X[:, :, ::specs['pooling'], :]\n",
    "#     # ),\n",
    "#     # tf.keras.layers.Dropout(specs['dropout'], noise_shape=None),\n",
    "#     # Dense(size=out_dim, nonlin=tf.identity, specs=specs)\n",
    "# )\n",
    "\n",
    "# newnetd = ModelDesign(\n",
    "#     tf.keras.Input(shape=(1, n_times, n_channels)),\n",
    "#     Deconw(kernel_size=(4, 10)),\n",
    "#     tf.keras.layers.Conv2D(1, (1, 10)),\n",
    "#     LayerDesign(\n",
    "#         lambda X: tf.transpose(tf.squeeze(X, axis=-1), (0, 1, 3, 2))\n",
    "#     ),\n",
    "#     tf.keras.layers.DepthwiseConv2D((204, 1), name='demixing'),\n",
    "#     LayerDesign(\n",
    "#         lambda X: X[:, :, ::2, :]\n",
    "#     ),\n",
    "#     tf.keras.layers.Dropout(specs['dropout'], noise_shape=None),\n",
    "#     Dense(size=out_dim, nonlin=tf.identity, specs=specs)\n",
    "# )\n",
    "\n",
    "\n",
    "newnetd = ModelDesign(\n",
    "    tf.keras.Input(shape=(1, n_times, n_channels)),\n",
    "    Deconw(kernel_size=(4, 10)),\n",
    "    LayerDesign(\n",
    "        lambda X: tf.transpose(tf.squeeze(X, axis=-1), (0, 1, 3, 2))\n",
    "    ),\n",
    "    tf.keras.layers.DepthwiseConv2D((1, 10)),\n",
    "    tf.keras.layers.DepthwiseConv2D((204, 1), name='demixing'),\n",
    "    LayerDesign(\n",
    "        lambda X: X[:, :, ::2, :]\n",
    "    ),\n",
    "    tf.keras.layers.Dropout(specs['dropout'], noise_shape=None),\n",
    "    Dense(size=out_dim, nonlin=tf.identity, specs=specs)\n",
    ")\n",
    "\n",
    "\n",
    "print('input_shape: ', (1, n_times, n_channels))\n",
    "newnetd().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_specs(specs: dict):\n",
    "    specs.setdefault('filter_length', 7)\n",
    "    specs.setdefault('n_latent', 4)\n",
    "    specs.setdefault('pooling', 4)\n",
    "    specs.setdefault('stride', 4)\n",
    "    specs.setdefault('padding', 'SAME')\n",
    "    specs.setdefault('pool_type', 'max')\n",
    "    specs.setdefault('nonlin', tf.nn.relu)\n",
    "    specs.setdefault('l1', 3e-4)\n",
    "    specs.setdefault('l2', 0)\n",
    "    specs.setdefault('l1_scope', ['fc', 'demix', 'lf_conv'])\n",
    "    specs.setdefault('l2_scope', [])\n",
    "    specs.setdefault('maxnorm_scope', [])\n",
    "    return specs\n",
    "\n",
    "\n",
    "LayerLike = Optional[list[Callable[[tf.Tensor], tf.Tensor]]]\n",
    "\n",
    "class NetworkBuilder(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        design_parts: LayerLike = None,\n",
    "        demixing_layer: LayerLike = None,\n",
    "        temporal_filtering_layer: LayerLike = None,\n",
    "        pooling_layer: LayerLike = None,\n",
    "        specs: Optional[dict] = None\n",
    "    ):\n",
    "        self.specs = initialize_specs(specs)\n",
    "        self.parts = design_parts if design_parts else list()\n",
    "        self.demixing = demixing_layer if demixing_layer else DeMixing(\n",
    "            size=self.specs['n_latent'],\n",
    "            nonlin=tf.identity,\n",
    "            axis=3, specs=self.specs\n",
    "        )\n",
    "        self.pooling = pooling_layer if pooling_layer else TempPooling(\n",
    "            pooling=self.specs['pooling'],\n",
    "            pool_type=self.specs['pool_type'],\n",
    "            stride=self.specs['stride'],\n",
    "            padding=self.specs['padding'],\n",
    "        )\n",
    "        self.temporal_filter = temporal_filtering_layer if temporal_filtering_layer else LFTConv(\n",
    "            size=self.specs['n_latent'],\n",
    "            nonlin=self.specs['nonlin'],\n",
    "            filter_length=self.specs['filter_length'],\n",
    "            padding=self.specs['padding'],\n",
    "            specs=self.specs\n",
    "        )\n",
    "\n",
    "    def add_temporal_filter(self):\n",
    "        self.parts.append(self.temporal_filter)\n",
    "\n",
    "    def add_demixing(self):\n",
    "        self.parts.append(self.demixing)\n",
    "\n",
    "    def add_pooling(self):\n",
    "        self.parts.append(self.pooling)\n",
    "\n",
    "    def add_layer(self, layer: LayerLike):\n",
    "        self.parts.append(layer)\n",
    "\n",
    "    def design(self):\n",
    "        return ModelDesign(\n",
    "            None, *self.parts\n",
    "        )\n",
    "\n",
    "\n",
    "class NetworkDirector(object):\n",
    "    def __init__(self, builder: NetworkBuilder, layers_dict: dict[str: LayerLike]):\n",
    "        self.builder = builder\n",
    "        self.layers_dict = layers_dict\n",
    "\n",
    "    def construct(self, config: str):\n",
    "        config = config.split('')\n",
    "        for part_config in config:\n",
    "            {\n",
    "                'f': self.builder.add_temporal_filter,\n",
    "                'd': self.builder.add_demixing,\n",
    "                'p': self.builder.add_pooling\n",
    "            }[part_config]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (1, 361, 204)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1, 361, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "newnetd = ModelDesign(\n",
    "    tf.keras.Input(shape=(1, n_times, n_channels)),\n",
    "    Deconw(kernel_size=(4, 10)),\n",
    "    LayerDesign(\n",
    "        lambda X: tf.transpose(tf.squeeze(X, axis=-1), (0, 1, 3, 2))\n",
    "    ),\n",
    "    tf.keras.layers.DepthwiseConv2D((204, 1), name='demixing'),\n",
    "    tf.keras.layers.DepthwiseConv2D((1, 10)),\n",
    "    # tf.keras.layers.DepthwiseConv2D((204, 1), name='demixing'),\n",
    "    # LayerDesign(\n",
    "    #     lambda X: X[:, :, ::2, :]\n",
    "    # ),\n",
    "    # tf.keras.layers.Dropout(specs['dropout'], noise_shape=None),\n",
    "    # Dense(size=out_dim, nonlin=tf.identity, specs=specs)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print('input_shape: ', (1, n_times, n_channels))\n",
    "newnetd().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.keras.Input(shape=(1, n_times, n_channels))\n",
    "model = tf.keras.Model(inp, newnetd(inp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25b6c1d617e3cb25e4067864bcd46322e1b7da41afdae0cf7c23b941b0b9b767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
